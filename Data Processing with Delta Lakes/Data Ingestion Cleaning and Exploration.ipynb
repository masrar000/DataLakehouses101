{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ffa84b-22df-42e3-8429-8fc5218fe526",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion, Cleaning and Exploration with Delta Lake \n",
    "\n",
    "Databricks provides a notebook interface compatible with Python, SQL, Pyspark, Scala, R, and more. In this notebook, we discuss how some basic data operations that can be performed on a table using Pyspark. We also introduce Delta Lake and Delta tables. \n",
    "\n",
    "All code and descriptions below are written by Zoya Shafique, unless where noted.\n",
    "\n",
    "## <img src = 'https://www.svgrepo.com/show/176852/pin-signs.svg' style=\"height: 50px; margin: 5px; padding: 5px\"/> Overview\n",
    "---\n",
    "\n",
    "In this tutorial, you'll learn how to use Databrick's Delta Lake and PySpark functionalities for handling data. This tutorial is intended for users with some experience with data handling, Python and Machine Learning. \n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "* Create and manage Delta Tables\n",
    "* Use PySpark for understanding and cleaning data\n",
    "\n",
    "Note that the goal of this tutorial is not to provide a walk through of data cleaning but rather to show how Databricks lakehouse storage, Delta Lake, can be used in combination with PySpark for data handling.\n",
    "\n",
    "## <img src = 'https://www.svgrepo.com/show/176852/pin-signs.svg' style=\"height: 50px; margin: 5px; padding: 5px\"/> Before you begin\n",
    "---\n",
    "Before you start the tutorial, you should:\n",
    "\n",
    "* Download the dataset from <a href=\"https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho\" target=\"_blank\">this link</a>.\n",
    "* Create a compute resource. For information on how to initialize your compute, plese <a href=\"https://github.com/zoyashaf/DataLakehouses101/blob/21011a4ffd7e4eb7f045f393720a428b940a3b3b/docs/create_compute.pdf\" target=\"_blank\">check here</a>.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798df82-980e-44fd-baf2-eeb329a7db87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### <img src='https://www.svgrepo.com/show/122877/presentation.svg' style=\"height: 65px; margin: 5px; padding: 5px\"/> Loading Data\n",
    "---\n",
    "This section covers basics of loading data in a notebook in Databricks. Users familiar with Pandas will see many similarities between the PySpark interface and Pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3168514c-8239-4d1d-901f-2bc97e96713f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To begin, we must first initialize our Spark session which will allow us to use the DataFrame API to handle our data. \n",
    "In line 8 we are creating our spark session instance. We can use the SparkSession.Builder object to configure our Spark session, \n",
    "but for the sake of this tutorial, we are using the default settings. \n",
    "'''\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bd40ad-2fbd-47cb-84d5-7ec523f3503e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src='https://www.svgrepo.com/show/530436/help.svg' style=\"height: 50px; margin: 0px; padding: 0px\"/> Please refer to <a href=\"https://github.com/zoyashaf/DataLakehouses101/blob/5e5423427db745aa64ea52e3efe7fcd1fb04a288/figures/catalog.png\" target=\"_blank\">this image</a> to see how to import data into your databricks account and notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132c49a3-cb3b-4188-9d9d-7ebb16dd912f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading in data from our file storage \n",
    "## .option tells our code to read in the header row of the csv file \n",
    "car_data = df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/zshafiq001@citymail.cuny.edu/car_details_v4_edited-1.csv\")\n",
    "display(car_data.limit(5))\n",
    "# NOTE: You may also use car_data.show(), however display() showcases the data in an easy to read table whereas show() provides a raw output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742a5e75-7169-4262-bf3d-100b0e54ac65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### <img src='https://www.svgrepo.com/show/170412/notebook.svg' style=\"height: 65px; margin: 5px; padding: 5px\"/> Task 1: \n",
    "\n",
    "\n",
    "Click on the '+' sign next to 'Table' in the cell above. You will see options for visualization and data profile. \n",
    "  * First generate a data profile. Do you notice anything weird about the data? \n",
    "  * Next, generate a bar graph using the visualization option. Use 'Make' as the x-axis and 'Price' as the y-axis. Does there seem to be a discernable pattern between makes of cars and their price?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eeffbcf-9789-4b93-a0e5-0a9b00f9eaa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "!!!\n",
    "< Your answer here > \n",
    "!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744e42f1-212e-4e18-a515-56786b62b881",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also use pyspark commands to learn more about our data*: \n",
    "* **`describe()`**:  displays count, mean, stddev, min, max. \n",
    "* **`summary()`**:  displays interquartile range (IQR) in addition to attributes from describe.\n",
    "* **`printschema()`**: prints table schema in a tree format, with each column name followed by the data type and the nullability indicator, which shows if the column allows nulls or not.\n",
    "\n",
    "\n",
    "*Note: adapted from DataBricks Academy ML 01- Data Cleansing tutorial \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce08f2c2-246f-4262-95df-ea6588332f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(car_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c1747ef9-abd8-4e7f-a34f-e5adfd036634",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(car_data.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1b4612ec-6d65-4dab-9d82-ca3c90f477f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "car_data.printSchema()\n",
    "# Since printSchema() is meant to print, we don't need to add a display wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c48505-11da-4f78-acb7-918fafb50b93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### <img src='https://upload.wikimedia.org/wikipedia/commons/6/68/Exclamation_Point.svg' style=\"height: 45px; margin: 5px; padding: 5px\"/> Concept Review\n",
    "\n",
    "* <b>Data Profile</b> allows users to quickly and easily gain an understanding of their data. The tool provides a complete overview of the dataset's characteristics, statistics, and more. Furthermore, the data profile along with any visualizations can easily be added to a dashboard to quickly create effective summaries of the data. With these features, users can easily understand the basic structure of their data, explore the data distribution, identify missing values and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29296ecd-f4b4-496b-b162-3bf0f8f7d19a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### <img src='https://www.svgrepo.com/show/229520/lake.svg' style=\"height: 80px; margin: 5px; padding: 5px\"/> Delta Lake Tables \n",
    "\n",
    "Currently, our data exists as a DataFrame that we built from our .csv file. To take full advantage of Databrick's software, we can convert our DataFrame into a Delta Lake table. Delta Lake is an open-source storage layer used by Databricks. It provides organization and ACID transaction support to traditional lake storage, such as a distributed file system. As such, saving our data as a Delta table can proide extra functionality for efficient processing. \n",
    "\n",
    "More information about Delta Lake can be found <a href=\"https://docs.databricks.com/en/delta/index.html\" target=\"_blank\">here. </a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd26a018-ab36-42e4-8522-7ed567bd02ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the data to a table.\n",
    "table_name = \"car_data_v2\"\n",
    "car_data.write.saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7cd592-dadb-4357-8550-8183086103fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## We can use a spark.sql query to get a quick overview of the table we created in the previous cell. The following command shows us the meta data associated with our table. \n",
    "display(spark.sql('DESCRIBE DETAIL car_data_v2'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d4f183-402f-42f0-b265-5c780fe8ad2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To view the created table, navigate to the catalog tab in the sidebar. You will see \"car_data_v1\" listed underneath tables in the \"Database Tables\" tab. You can find more information about how to view your table in the catalog here. One of the main advantages of using a Delta table as opposed to a DataFrame is that the Delta Table keeps a historical record of your data. This helps with data versioning and also reproducibility. Fruthermore, a historical record can help you monitor your data quality and keep track of any and all changes. The best part in all of this is that, Delta Lake is an extension of the Spark DataFrame API, so we can treat our Delta table just like a normal table if we choose to. For more information, <a href=\"https://github.com/zoyashaf/DataLakehouses101/blob/a6aecacba9a429d2eca83c2442928067ac9e9001/docs/Delta%20Lake%20Tables.pdf\" target=\"_blank\"> please see the associated documentation.  </a> \n",
    "\n",
    "\n",
    "\n",
    "Another advantage of storing our DataFrame as a Delta table is that it allows us to store and manage metadata along with our actual data. \n",
    "\n",
    "More information about the functionality of Delta tables can be found <a href=\"https://docs.databricks.com/en/delta/tutorial.html#create-a-table\" target=\"_blank\">here. </a> \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60db1c47-b102-4028-a594-58250c87a99e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### <img src = 'https://www.svgrepo.com/show/503651/vacuum-cleaner.svg' style=\"height: 80px; margin: 5px; padding: 5px\"/> Data Cleaning \n",
    "---\n",
    "In this section, we will explore how we can use Delta tables and PySpark has for analyzing and cleaning data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0db904-eb05-4805-a219-b8c74b0fb7de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First lets load in our Delta table \n",
    "## Note: We can load our table using the table name we specified earlier or by using the direct path to the table. \n",
    "car_table = spark.read.table(table_name)\n",
    "display(car_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402523f7-a919-4611-af39-3718ef564e75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### <img src = 'https://www.svgrepo.com/show/499853/idea.svg' style=\"height: 60px; margin: 5px; padding: 5px\"/> Looking at Datatypes\n",
    "\n",
    "As we see from our data profile and our describe method above, many of the numerical categories were picked up as strings. We need to fix this before we can use our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8541164e-ca76-4524-8ee8-f1534904ce06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, translate\n",
    "\n",
    "fixed_price_df = car_table.withColumn(\"price\", translate(col(\"price\"), \" \", \"\").cast(\"double\"))\n",
    "\n",
    "## Lets confirm if the change worked as expected \n",
    "fixed_price_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1b3bcd-241d-46db-93e1-003eedb199d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### <img src='https://www.svgrepo.com/show/170412/notebook.svg' style=\"height: 65px; margin: 5px; padding: 5px\"/> Task 2: \n",
    "Which other columns should be numerical but were read as strings? Convert these to the correct data type. \n",
    "  * Hint: Some columns do contain characters alongside numerical values. For these columns, use translate(col(\"Column Name\", \"Characters\", \"\")) to remove them before casting the column as a numerical datatype\n",
    "  * Consider: Can we simply apply col and translate to the 'Max Power' and 'Max Torque' columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df2ef87-3ecd-47c5-9665-ea1d9c351e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!!! Your answer here !!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba08007-f707-4a5f-b8a4-21531521f1d9",
     "showTitle": true,
     "title": "Solution"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## NOTE: This is just one solution. \n",
    "from pyspark.sql.functions import split #function used for splitting strings \n",
    "\n",
    "## Separting Max Torque and Max Power columns into their parts \n",
    "### First, we split the strings at '@'. Then we assign each part of the split string to a separate column. Finally, we drop the original column from the dataframe. \n",
    "\n",
    "fixed_dtype_df = fixed_price_df.withColumn(\"max_torque_Nm\", split(\"max_torque\", \"@\")[0]) \\\n",
    "                   .withColumn(\"max_torque_rpm\", split(\"max_torque\", \"@\")[1])\n",
    "fixed_dtype_df = fixed_dtype_df.drop(\"Max Torque\")\n",
    "\n",
    "fixed_dtype_df = fixed_dtype_df.withColumn(\"max_power_bhp\", split(\"max_power\", \"@\")[0]) \\\n",
    "                   .withColumn(\"max_power_rpm\", split(\"max_power\", \"@\")[1])\n",
    "fixed_dtype_df = fixed_dtype_df.drop(\"max_power\")\n",
    "\n",
    "## After splitting the columns, we need to remove the units from the rows \n",
    "columns_with_strings = [['max_power_bhp', ' bhp', 'double'],\n",
    "                  ['max_power_rpm', ' rpm', 'double'],  ['max_torque_rpm', ' rpm', 'double'], ['max_torque_Nm', ' Nm', 'double']]\n",
    "\n",
    "for column, string, dtype in columns_with_strings:\n",
    "  fixed_dtype_df = fixed_dtype_df.withColumn(column, translate(col(column), string, '').cast(dtype))\n",
    "\n",
    "## We use translate() to replace 'cc' with empty strings and then convert the column to the correct data type. \n",
    "fixed_dtype_df = fixed_dtype_df.withColumn(\"engine_cc\", translate(col(\"engine\"), \" cc\", \"\").cast(\"double\")) \n",
    "fixed_dtype_df = fixed_dtype_df.drop(\"engine\")\n",
    "\n",
    "## Converting string columns to int/double types\n",
    "columns_to_fix = [['year', 'int'], ['kilometer', 'double'], ['length', 'double'],\n",
    "                  ['width', 'double'], ['height', 'double'], ['seating_capacity', 'int'],\n",
    "                  ['fuel_tank_capacity', 'double']] \n",
    "\n",
    "for column, dtype in columns_to_fix:\n",
    "  fixed_dtype_df = fixed_dtype_df.withColumn(column, translate(col(column), ' ', '').cast(dtype))\n",
    "\n",
    "fixed_dtype_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69653f81-5570-47a3-987a-723ec66eadeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now that our data is in the correct format, lets recalculate the statistics:\n",
    "display(fixed_dtype_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5a6cc6-17b7-47d0-a2f2-46481e634666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Now lets save this data with a note that we fixed the Schema\n",
    "fixed_dtype_df.write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-dtypes-in-columns\") \\\n",
    "  .saveAsTable(\"default.car_data_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45af641f-669c-4108-a9ba-0e903ce8b915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the above command, we are saving another copy of our data on top of the original file.\n",
    "\n",
    "* **`.option(\"overwriteSchema\", \"true\") `**:  replaces the existing schema with the updated one. We need this as we changed the datatypes of the columns, which is recorded as part of the schema. \n",
    "* **`.option(\"userMetadata\", <comment>)`**: used to add messages to our commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b25f7a5-f00a-481d-b596-4ba7d4ab167e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####  <img src = 'https://www.svgrepo.com/show/499853/idea.svg' style=\"height: 60px; margin: 5px; padding: 5px\"/> Handling extreme and null values \n",
    "\n",
    "##### Looking into extreme values \n",
    "From our describe functions above, we can see some strange data such as a minimum of 0 km in the Kilometer column as a Fuel Capacity of 15. A max price of 3.5e7 also seems extreme for a used car. Let's explore this more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9377a1ae-1029-4c46-90d7-987490753ba5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets take a look at the price column \n",
    "display(fixed_dtype_df\n",
    "        .groupBy(\"price\").count()\n",
    "        .orderBy(col(\"price\").desc(), col(\"count\"))\n",
    "       )\n",
    "\n",
    "## NOTE: Adding a visualization to our table  here can help us quickly understand the distribution of our data and the outliers \n",
    "### Some cars seem very expensive compared to the majority of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d21d3e2-45ef-4584-9c0d-2643e8ab2472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fixed_dtype_df.filter(col(\"kilometer\") == 0))\n",
    "# Mini coopers can be expensive, the year of the model is 2022 and the car is unregistered so perhaps it is a new car for sale? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59062181-2ce6-4d55-95f2-89c6ef306aff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pos_km_df = fixed_dtype_df.filter(col(\"kilometer\") > 0) # only keeping rows with km greater than 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e57f6e2-2895-4fbb-ad60-4cff61be322e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now lets take a look at the minimum maximum values\n",
    "display(pos_km_df\n",
    "        .groupBy(\"kilometer\").count()\n",
    "        .orderBy(col(\"kilometer\").desc(), col(\"count\"))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691013fd-9832-4df7-b38b-092c7c5d0682",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### <img src='https://www.svgrepo.com/show/170412/notebook.svg' style=\"height: 65px; margin: 5px; padding: 5px\"/> Task 3: \n",
    "\n",
    "\n",
    "Click on the '+' sign next to 'Table' in the cell above. You will see options for visualization.\n",
    "  * Create two histogram plots. One with a bin size of 10 and one with a bin size of 100. What do these vizualizations tell you about the data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5246b948-2a5a-4645-b67f-9ac34aaa58e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "!!!\n",
    "< Your answer here > \n",
    "!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7a30fa-5b10-49ae-9110-d47bb095f89c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Two used cars with 1 km (a litte more than 0.5 miles) seems strange. Lets look into the row further\n",
    "display(pos_km_df.filter(col(\"kilometer\") == 1))\n",
    "# These are both 2022 Audis. Considering the high price, perhaps these are also new cars for sale? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636166da-a972-4c27-a469-834011246546",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Looking into null values \n",
    "We also have many columns with null values. How we approach these depends greatly on the domain and task at hand. \n",
    "\n",
    "For a moment, lets consider a different example. For instance, a survey dataset where respondents were asked about their income level, education level, and whether they own a car. In this dataset, the \"car ownership\" column contains null values for some respondents. Some key considerations for this dataset are listed below. \n",
    "\n",
    "  * <b> Missing Data: </b> Null values in the \"car ownership\" column could indicate missing data or non-response. This could be due to various reasons such as respondents choosing not to answer the question, data entry errors, or survey design issues. Understanding the missing data mechanism is crucial for assessing data quality and potential biases in the dataset.\n",
    "  * <b> Imputation Strategy: </b> The presence of null values in the \"car ownership\" column could influence the choice of imputation strategy if the goal is to fill in missing values. For example, if null values are more prevalent among respondents with lower income levels, simply imputing the mean or median car ownership rate may not be appropriate as it could bias the analysis.\n",
    "  * <b> Analyzing Patterns: </b> If null values in the \"car ownership\" column are associated with certain demographic characteristics such as age or location, it could indicate differences in car ownership rates among different groups of respondents. Understanding these patterns could provide valuable insights for targeted marketing strategies or policy interventions.\n",
    "\n",
    "Depending on the context, how we handle null values chaneges. Some approaches are *\n",
    "* Drop all rows with null values \n",
    "* Replace them with mean/median/zero/etc. \n",
    "* Replace them with the mode \n",
    "* Create a new column to denote rows that have null values \n",
    "\n",
    "For the purposes of this tutorial, we will replace the missing values with a value such as the average. This process is known as imputing. For this, we will look at Spark's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html?highlight=imputer#pyspark.ml.feature.Imputer\" target=\"_blank\"><b> Imputer </b></a>  method. Note, it is important to include an extra column denoting that a field has been imputed if the operation is performed*.\n",
    "\n",
    "\n",
    "*Note: adapted from DataBricks Academy ML 01- Data Cleansing tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b479bd8e-1910-46e7-856e-a8864879f8ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#### <img src='https://www.svgrepo.com/show/170412/notebook.svg' style=\"height: 65px; margin: 5px; padding: 5px\"/> Task 4: \n",
    "\n",
    "\n",
    "Take a moment to look through the Imputer function's documentation (linked above).\n",
    "  * Is there any requirements for the data type of the inputs? \n",
    "  * Can Imputer perform any type of imputation (e.g., numerical, categorial)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55059459-3c61-402e-bfa5-71e55d219800",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "!!!\n",
    "< Your answer here > \n",
    "!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b146c0f6-7968-4fc5-a9e4-9f2b0afa3c78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the following cells, we will prepare our data for imputing. \n",
    "  * First, we need to convert any integer columns into double \n",
    "  * We need to denote rows where null values are present so we can keep track of imputed values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfd9e82-3fe5-4422-9589-6b6e17c22ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The code in this cell is taken from: Databricks Academy ML 01 Data Cleansing \n",
    "'''\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "integer_columns = [x.name for x in pos_km_df.schema.fields if x.dataType == IntegerType()]\n",
    "doubles_df = pos_km_df\n",
    "\n",
    "for c in integer_columns:\n",
    "    doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "columns = \"\\n - \".join(integer_columns)\n",
    "print(f\"Columns converted from Integer to Double:\\n - {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9122cc-6740-4dcb-8faf-843899867154",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to denote which rows had null values before we impute our data. \n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "impute_cols = [\n",
    "    \"kilometer\",\n",
    "    \"max_power_bhp\", \n",
    "    \"max_torque_Nm\",\n",
    "    \"max_power_rpm\", \n",
    "    \"max_torque_rpm\",\n",
    "    \"length\",\n",
    "    \"width\",\n",
    "    \"height\",\n",
    "    \"seating_capacity\",\n",
    "    \"fuel_tank_capacity\",\n",
    "    \"engine_cc\"\n",
    "]\n",
    "\n",
    "# We will put a 0 if there is no value in the given column for that row and a 1 if there is a null value. \n",
    "for c in impute_cols:\n",
    "    doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c9d5c4-2fec-4f3d-b678-55d6cce38469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(doubles_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15fe9a76-9317-43b9-a111-fde56dcd1825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We are now ready to impute our data! Users familiar with scikit-learn will recognize the syntax for applying the Imputer function. \n",
    " * We first create an instance of the Imputer object, specifying the method that we want to use to impute our data. \n",
    " * Next, we 'fit' the impute instance on our data.\n",
    " * Finally, we call Imputer's transform method to convert our existing dataframe with its null values into a dataframe with all the null values filled in. \n",
    "\n",
    "More speciically, Spark ML's APIs are standardized in much the same way as scikit-learn. This allows different methods to be packaged into one pipeline. More details on two of the key components of the Spark ML API are described below: \n",
    "\n",
    "**\n",
    "* **Transformers**: Converts one DataFrame into another. Takes a DataFrame as input and returns an updated DataFrame, based on the function. Transformers do not learn any parameters from the data and simply apply rule-based transformations. It has a **`.transform()`** method. \n",
    "\n",
    "* **Estimator**: An algorithm which can be fit on a DataFrame to produce a Transformer. It has a **`.fit()`** method because it learns parameters from your DataFrame in order to transform it.   \n",
    "\n",
    "**\n",
    "\n",
    "It is important to note that any call to a '.fit()' function should only be applied to training data. \n",
    "\n",
    "** Note: Descriptions taken from DataBricks Academy ML 01- Data Cleansing tutorial \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be7d101-1917-45d6-8108-2d3159ac5c01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\", inputCols=impute_cols, outputCols=impute_cols)\n",
    "\n",
    "imputer = imputer.fit(doubles_df)\n",
    "imputed_df = imputer.transform(doubles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558e89b4-9c50-44a3-b724-73d57fbb1759",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## lets display our imputed data \n",
    "display(imputed_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e498d543-9d45-459d-b0b0-c331d0bd0eca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Lets take another look at the summary statistics for the dataset. \n",
    "display(imputed_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f5bddc-4e46-4286-81b4-cd10d31a9e72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Consider: We could also have approached this problem by calculating statistics based on Make and using those statistics to fill in null values for relevant fields. What other ways could we have approached this problem? \n",
    "\n",
    "\n",
    "Now that our data is cleaned, we can save our DataFrame to the Delta Lake. As our table already exists as a Delta table, we can use the 'overwrite' flag when saving our data to ensure that the existing file is replaced with our updated version. Saving our cleaned data to Delta Lake ensures that the data is stored in a reliable and efficient manner, making it ready for subsequent analysis, including building machine learning models. We will not have to save it to our local machine and reupload it again but can access it directly from our catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0daa249a-eb14-4733-9bbd-c6c3f476910b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## As we made changes to our schema (i.e., we changed the data type of some columns), we need to include th overwriteSchema command to write our updated table. \n",
    "\n",
    "imputed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.car_data_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a17348-4658-497c-b866-b2ec80cb0072",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If we go back to our catalog and view the history of our table, we can see that a new version has been added, denoting all of the changes we made. This is a more compact and efficient way to process data than saving new files for all updated tables as it allows us to easily track changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "058fa676-4c7e-43ef-a1c8-eab1479b3afb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## <img src = 'https://www.svgrepo.com/show/176852/pin-signs.svg' style=\"height: 50px; margin: 5px; padding: 5px\"/> Summary\n",
    "---\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "* Create Delta tables\n",
    "* Use Delta tables with PySpark to clean data \n",
    "* Update your tables to maintain a consistent record of your data\n",
    "\n",
    "\n",
    "## <img src = 'https://www.svgrepo.com/show/199671/next.svg' style=\"height: 50px; margin: 5px; padding: 5px\"/> Next steps\n",
    "---\n",
    "\n",
    "Take a look into the docs for:\n",
    "*\n",
    "* <a href=\"https://github.com/zoyashaf/DataLakehouses101/blob/5736beeea55e96813750e154f69e37c7ca0e6de0/docs/Delta%20Lake%20Tables.pdf\" target=\"_blank\">  A Look at the Delta Table Interface  </a> \n",
    "\n",
    "Or continue to the next tutorial: \n",
    "* <a href=\"https://github.com/zoyashaf/DataLakehouses101/blob/5736beeea55e96813750e154f69e37c7ca0e6de0/Data%20Processing%20with%20Delta%20Lakes/Delta%20Lake%20Tutorial.ipynb\" target=\"_blank\"> Delta Lake Tutorial </a> \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "132c49a3-cb3b-4188-9d9d-7ebb16dd912f",
       "elementType": "command",
       "guid": "02d3267f-d437-460a-a856-a4f7fddf4b0c",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "39c02ff4-eab9-48c1-a7ff-7cbc8881af57",
     "origId": 580693255211612,
     "title": "demo",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2339562404919506,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Ingestion Cleaning and Exploration",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
